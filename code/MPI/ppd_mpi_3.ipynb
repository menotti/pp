{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_vqFo-wFgJn"
   },
   "source": [
    "# PPD: MPI e programação com passagem de mensagem\n",
    "\n",
    "Hélio - DC/UFSCar - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqLeez1BbscH"
   },
   "source": [
    "# Comunicações coletivas\n",
    "\n",
    "Algumas formas de comunicação oferecidas por MPI são operações coletivas (*Collective communications*) e envolvem **todos** os processos pertencentes a um grupo (*communicator*) de uma só vez. \n",
    "\n",
    "Nessas operações, a mesma chamada deve ser emitida por todos os processos do grupo envolvido, sendo que um ou mais processos irão enviar dados e um ou mais irão recebê-los. Cabe ao programador, contudo, garantir que todos os processos do grupo emitam as chamadas apropriadas. \n",
    "\n",
    "Tipos de operações coletivas (*Collective Operations*):\n",
    "\n",
    "* ***Synchronization***: operações coletivas deste tipo servem para que processos esperem até que todos os membros do grupo atinjam um ponto de sincronização;\n",
    "* ***Data Movement***: permitem a transferência de dados, em diferentes modelos (*broadcast, scatter/gather, all to all, ...*);\n",
    "* ***Collective Computation (reductions)***: são operações coletivas em que um membro do grupo coleta os dados e realiza uma operação sobre eles, como nas operações de redução (min, max, add, multiply, etc.)\n",
    "\n",
    "Considerações:\n",
    "\n",
    "* Como envolvem todos os processos em um grupo (*communicator*) operações coletivas são bloqueantes e têm o efeito de sincronizar todos os processos do grupo.\n",
    "* Não há *tags* nas mensagens envolvidas nessas operações.\n",
    "* Para realizar operações coletivas usando apenas sub-conjuntos dos processos da aplicação, é preciso antes criar os sub-grupos (*communicators*) apropriados.\n",
    "* Apenas tipos pré-definidos por MPI podem ser usados nas comunicações coletivas.\n",
    "\n",
    "Primitivas para comunicações coletivas:\n",
    "\n",
    "* **MPI_Barrier** (comm): Cria uma barreira de sincronização para os processo de um grupo. Todos os processos que realizam a chamada são bloqueados até que o último processo do grupo faça esta mesma chamada. \n",
    "* **MPI_Bcast** (&buffer,count,datatype,root,comm): o processo cujo rank for especificado como *root* na chamada difunde (*broadcasts*) uma mensagem para todos os demais do grupo.\n",
    "* **MPI_Scatter** (&sendbuf,sendcnt,sendtype,&recvbuf,...... recvcnt,recvtype,root,comm): o processo identificado como *root* na chamada distribui partes distintas da mensagem para os demais processos do grupo. \n",
    "* **MPI_Scatterv** ( ): \n",
    "* **MPI_Gather** (&sendbuf,sendcnt,sendtype,&recvbuf, ...... recvcount,recvtype,root,comm): de forma contrária ao que ocorre em *MPI_Scatter*, nesta chamada, o processo identificado como *root* coleta (*gathers*) mensagens distintas de todos os demais nós do grupo, concatenando-as em posições distintas do buffer de recepção, de acordo com o número lógico dos processos emissores.\n",
    "* **MPI_Gatherv**( ):\n",
    "* **MPI_Allgather** (&sendbuf,sendcount,sendtype,&recvbuf, ...... recvcount,recvtype,comm): nessa operaçao de concatenação, todos os processos do grupo obtêm os dados transmitidos por todos no grupo.\n",
    "\n",
    "Nas operações coletivas de **redução**, além de coletar os dados enviados por todos os processos do grupo, o processo *root* realiza uma operação de agregação sobre os dados.\n",
    "\n",
    "* **MPI_Reduce** (&sendbuf,&recvbuf,count,datatype,op,root,comm): processo *root* coleta todos os dados e aplica uma operação de redução.\n",
    "\n",
    "* **MPI_Allreduce** (const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm): também realiza a redução, mas todos os processos do grupo têm o resultado.\n",
    "\n",
    "* **MPI_Reduce_scatter** (&sendbuf,&recvbuf,recvcount,datatype, ...... op,comm): *First does an element-wise reduction on a vector across all tasks in the group. Next, the result vector is split into disjoint segments and distributed across the tasks. This is equivalent to an MPI_Reduce followed by an MPI_Scatter operation*.\n",
    "\n",
    "* **MPI_Alltoall** (&sendbuf,sendcount,sendtype,&recvbuf, ...... recvcnt,recvtype,comm): *Each task in a group performs a scatter operation, sending a distinct message to all the tasks in the group in order by index*.\n",
    "\n",
    "* **[MPI_Scan](https://www.open-mpi.org/doc/v3.1/man3/MPI_Scan.3.php)**(&sendbuf,&recvbuf,count,datatype,op,comm): *Performs a scan operation with respect to a reduction operation across a task group*.\n",
    "\n",
    "```\n",
    "MPI Reduction   Operation                 C Data Types\n",
    "MPI_MAX         maximum                   integer, float\n",
    "MPI_MIN         minimum                   integer, float\n",
    "MPI_SUM         sum                       integer, float\n",
    "MPI_PROD        product                   integer, float\n",
    "MPI_LAND        logical AND               integer\n",
    "MPI_BAND        bit-wise AND              integer\n",
    "MPI_LOR         logical OR                integer \n",
    "MPI_BOR         bit-wise OR               integer, MPI_BYTE\n",
    "MPI_LXOR        logical XOR               integer\n",
    "MPI_BXOR        bit-wise XOR              integer, MPI_BYTE\n",
    "MPI_MAXLOC      max value and location    float, double and long double\n",
    "MPI_MINLOC      min value and location    float, double and long double\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb9WsW6XVBeC"
   },
   "source": [
    "# Broadcast\n",
    "\n",
    "Ao criar aplicações que se comunicam via MPI, independentemente da tecnologia da rede física e dos protocolos usados na interligação dos computadores,  não é preciso preocupar-se com detalhes das transmissões. Não é preciso ficar cuidando de quais são os processos que compõem a aplicação, de quais são os endereços IP dos computadores em que esses processos estão sendo executados, dos números de porta utilizados, de detalhes de *buffers* e retransmissões. Tudo isso é feito pela implementação MPI.\n",
    "\n",
    " Por exemplo, nos casos em que é preciso **enviar a mesma informação** para **todos** os processos da aplicação (que estão no mesmo MPI_COMM_WORLD), há 2 formas: replicando envios individuais, ou usando [MPI_Bcast](https://www.open-mpi.org/doc/v3.1/man3/MPI_Bcast.3.php).\n",
    "```\n",
    "1. // Broadcast com envios individuais\n",
    "  int root = 0;\n",
    "  ...\n",
    "  MPI_Comm_size(MPI_COMM_WORLD,&num_tasks);\n",
    "\n",
    "  if (rank == root)\n",
    "    for (i = 1; i < num_taks; i++) {\n",
    "      // int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
    "      //              int tag, MPI_Comm comm)\n",
    "      MPI_Send(tx_buf, count, data_type, i, tag, MPI_COMM_WORLD);\n",
    "  else\n",
    "    // int MPI_Recv(void *buf, int count, MPI_Datatype dtype, int src, \n",
    "    //              int tag, MPI_Comm comm, MPI_Status *stat)\n",
    "    MPI_Recv(rx_buf, count, data_type, root, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "```\n",
    "No caso 1, a aplicação identificou quantos processos estão sendo usados nesta execução e realizou envios individuais, replicando a mesma mensagem para todos. Nas transmissões, o índice do comando de iteração (***i***) foi usado para identificar o destino de cada mensagem, variando de ***1 a num_taks -1***.\n",
    "\n",
    "Todos os demais processos, com ***rank*** **> 0**, executaram uma operação de recebimento correspondente às mensagens lhes enviadas pelo proceso de *rank* **0** .\n",
    "\n",
    "<br>\n",
    "\n",
    "A segunda forma de enviar um mesmo conteúdo para todos os processos de um grupo é o uso da operação coletiva de ***broadcast***, ilustrado a seguir.\n",
    "\n",
    "```\n",
    "2.  // TODOS os processos do grupo (MPI_COMM_WORLD, neste caso), \n",
    "    // independentemente de seus ranks, têm que fazer a chamada.\n",
    "\n",
    "    // O processo root tem os dados no buf. \n",
    "    // Nos demais, buf é o endereço onde os dados recebidos serão copiados. \n",
    "\n",
    "    // int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm) \n",
    "    MPI_Bcast( buf, count, data_type, root, MPI_COMM_WORLD);\n",
    "```\n",
    "No caso 2, a aplicação realiza a difusão do conteúdo do *buffer* usando uma única chamada de *broadcast*. Nesta operação, **um** dos processos do grupo vai fazer a **transmissão** e **todos os demais** membros deste grupo vão **recebê-la**. \n",
    "\n",
    "Se o grupo especificado na chamada for MPI_COMM_WOLRD, isso significa que todos os processos da aplicação vão ter que executar a mesma chamada.\n",
    "\n",
    "Nesta chamada, o processo emissor é identificado pelo ***rank*** informado no parâmetro ***root***.  \n",
    "\n",
    "<br>\n",
    "\n",
    "Como todos os processos do grupo realizam a mesma chamada, é preciso atentar para o endereço passado no parâmetro *buffer*. Para o nó *root*, que é o emissor, o *buffer* aponta para o endereço de onde os dados a serem transmitidos estão posicionados na memória deste processo. Para os demais processos, passa-se o endereço da posição de memória **onde eles serão copiados**. \n",
    "\n",
    "Se o modelo de execução SMPD for usado na ativação do programa MPI, em que o mesmo código é executado em todos os processos, é preciso diferenciar os papeis que os processos com os diferentes *ranks* irão realizar.\n",
    "\n",
    "<br>\n",
    "\n",
    "Será que há alguma vantagem em usar o caso 2? Isso depende da tecnologia de transmissão usada na rede (Ethernet, e.g.).\n",
    "\n",
    "É claro que, nesta forma de transmissão, cabe à **implementação MPI** fazer com que a mesnagem seja efetivamente enviada a todos os processos do grupo. Se isso será feito com UDP ou TCP, usando *Ethernet broadcast*, ou mensagens replicadas, (felizmente :-) não é problema da aplicação. \n",
    "\n",
    "<br>\n",
    "\n",
    "Vejamos um exemplo a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBoMQ9wRCOxf",
    "outputId": "e3c8a302-aceb-428b-c565-cf18462456d8"
   },
   "outputs": [],
   "source": [
    "%%writefile bcast.c\n",
    "\n",
    "#include <sys/types.h>\n",
    "#include <unistd.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "#define N 10\n",
    "\n",
    "#define root 0\n",
    "\n",
    "int A[N], B[N], C[N];\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint i, rank, result, numtasks;\n",
    "\n",
    "\tresult = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (result != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
    "\t}\n",
    "\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "  // inicia semente do gerador de números aleatórios\n",
    "\t// srand(time(NULL));\n",
    "  srand(getpid());\n",
    "  \n",
    "\t// Todos iniciam vetor A\n",
    "\tfor (i=0; i < N; i++)\n",
    "\t\tA[i]=rand() % 10;\n",
    "\n",
    "\t// Processo com rank 0 foi escolhido para gerar vetor B e propagá-lo aos demais\n",
    "\tif(rank==root) \n",
    "\t\tfor (i=0; i < N; i++)\n",
    "\t\t\tB[i]=rand() % 10;\n",
    "\n",
    "\t// Todos os processos participam do broacast. \n",
    "\t// E claro que essa chamada poderia estar em trechos distintos do codigo, \n",
    "  // usando buffers distintos.\n",
    "\t// O que importa é que todos facam a chamada e, exceto pelo root, \n",
    "  // tenham espaco para armazenar os dados.\n",
    "\t// O processo root, 0 neste caso, tem o vetor cujo conteúdo sejá enviado\n",
    " \n",
    "\tMPI_Bcast( B, N, MPI_INT, root, MPI_COMM_WORLD); \n",
    "\n",
    "  printf(\"(%d) B: \",rank);\n",
    "\tfor(i=0; i < N; i++) \n",
    "\t\tprintf(\"%d \",B[i]);\n",
    "\tprintf(\"\\n\"); fflush(stdout);\n",
    " \n",
    "\t// todos fazem a operacao usando o mesmo vetor B\n",
    "\tfor(i=0;i<N;i++)\n",
    "\t\tC[i]=A[i]+B[i];\n",
    "\t// ...\n",
    "\n",
    "\t// todos imprimem o vetor C. \n",
    "\t// Na ativaçãodos processos remotos com mpirun, stdout é redirecionado para\n",
    "\t// o nó de origem. Assim, todas as impressões vão aparecer no terminal. \t\t\n",
    "  printf(\"(%d) C: \",rank);\n",
    "\tfor(i=0; i < N; i++) \n",
    "\t\tprintf(\"%d \",C[i]);\n",
    "\tprintf(\"\\n\"); fflush(stdout);\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KjHhDG9Tj-8",
    "outputId": "adaaabaf-0c9e-42de-db57-aef8036c0bef"
   },
   "outputs": [],
   "source": [
    "# ! if [ ! bcast -nt bcast.c ]; then mpiicc -Wall bcast.c -o bcast; fi && mpirun -n 4 -host localhost:4 bcast\n",
    "# !./compile.sh bcast \n",
    "# !./launch.sh bcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhDygKaQe2pH"
   },
   "source": [
    "# Scatter & Gatther\n",
    "\n",
    "Dentre as operações MPI para comunicação coletiva, a primitiva [MPI_Scatter](https://www.open-mpi.org/doc/v3.1/man3/MPI_Scatter.3.php) causa o **envio particionado** de uma mensagem a **todos** os processos de um grupo (*communicator*). \n",
    "\n",
    "```\n",
    "int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "                void *recvbuf, int recvcount, MPI_Datatype recvtype, \n",
    "                int root, MPI_Comm comm);\n",
    "int MPI_Iscatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "                 void *recvbuf, int recvcount, MPI_Datatype recvtype, \n",
    "                 int root, MPI_Comm comm, MPI_Request *request);\n",
    "```\n",
    "O efeito dessa primitiva, que faz o oposto de MPI_Gather, é como se uma série de transmissões fossem feitas com MPI_Send, cada uma enviando uma parte sequencial do conteúdo do *buffer* de transmissão para um processo separado: \n",
    "```\n",
    "MPI_Send(sendbuf + i * sendcount * sizeof(sendtype), sendcount, sendtype, i, ...);\n",
    "```\n",
    "com cada processo (incluse o emissor) executando uma operação de recebimento equivalente:\n",
    "``` \n",
    "MPI_Recv(recvbuf, recvcount, recvtype, i, ...).\n",
    "```\n",
    "\n",
    "O resultado é que cada processo do grupo vai **receber uma fração** da mensagem original, contendo um número de elementos **proporcional** à divisão do número total pelo número de processos do grupo. É como se a mensagem original fosse particionada e o **segemento** ***i*** fosse enviado ao processo ***i***.\n",
    "\n",
    "Examinando os campos desta primitiva, vê-se que há um *buffer* de envio e que há também indicação de um processo (***rank***) que será o ***root***, ou seja, o emissor nesta transmissão. No processamento da chamada, o *buffer* de envio (***sendbuf***) é ignorado em todos os nós cujo *rank* for diferente do ***root***.\n",
    "\n",
    "Outroa campos da primitiva incluem um ponteiro para o *buffer* de recebimento. Esse campo deverá ser válido **em todos** os processos (*ranks*), pois todos irão receber parte dos dados. O nó emissor também recebe sua fração dos dados.\n",
    "\n",
    "É claro que os campos ***root*** e ***comm*** devem conter os mesmos valores em todos \n",
    "os nós.\n",
    "\n",
    "\n",
    "\n",
    "# MPI_Gather\n",
    "\n",
    "[MPI_Gather](https://www.open-mpi.org/doc/v3.1/man3/MPI_Gather.3.php) tem o efeito contrário da operação *scatter*. Assim, o resultado é que o nó *root* vai receber uma mensagem de cada processo do grupo, inclusive uma sua, e vai colocar essas mensagens em sequência no buffer de recepção, em ordem correspondente aos identificadores dos nós no grupo.\n",
    "\n",
    "```\n",
    "int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "    void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,\n",
    "    MPI_Comm comm)\n",
    "```\n",
    "\n",
    "O programa exemplo a seguir ilustra o uso das operações de ***scatter*** e ***gather***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5oKx93wVzip",
    "outputId": "62ef5e22-c7dd-4f7e-8b8f-c50fba88d56e"
   },
   "outputs": [],
   "source": [
    "%%writefile scatter.c\n",
    "\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define SIZE 10\n",
    "\n",
    "int \n",
    "main(int argc, char *argv[])\n",
    "{\n",
    "\tint numtasks, rank, i, j;\n",
    "\n",
    "\tint recbuf[SIZE];\n",
    "\tint *fulbuf;\n",
    "\n",
    "\tchar hostname[MPI_MAX_PROCESSOR_NAME];\n",
    "\tint namelen;\n",
    "\n",
    "\tMPI_Init(&argc,&argv);\n",
    "\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
    "\n",
    "\tMPI_Get_processor_name(hostname, &namelen);\n",
    "\n",
    "\t// root node \n",
    "\tif(rank == 0) {\n",
    "\n",
    "\t\tfulbuf = (int *)malloc(numtasks * SIZE * sizeof(int));\n",
    "\n",
    "\t\t// preenche vetor para envio\n",
    "\t\tfor(i=0; i < numtasks; i++)\n",
    "\t\t\tfor(j=0; j < SIZE; j++)\n",
    "\t\t\t\tfulbuf [i*SIZE + j] = i;\n",
    "\t}\n",
    "\t// Todos os processos, independentemente de seus ranks, devem emitir essa \n",
    "  // chamada de operaçào coletiva. Os dados do processo root é que serão\n",
    "  // distribuídos, de forma fracionada, entre todos.\n",
    " \n",
    "\t// int MPI_Scatter(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "\t//                 void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,\n",
    "\t//                 MPI_Comm comm);\n",
    "\n",
    "\tMPI_Scatter(fulbuf, SIZE, MPI_INT, recbuf, SIZE, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "\t// todos recebem, inclusive o processo root\n",
    "\tprintf(\"%s (%d): \", hostname, rank);\n",
    "\tfor(i=0;i<SIZE;i++) {\n",
    "\t\tprintf(\"%d \",recbuf[i]); \n",
    "\t\trecbuf[i] += 10 * rank;\n",
    "\t}\n",
    "\tprintf(\"\\n\"); fflush(stdout);\n",
    "\t\n",
    "\t// int MPI_Gather(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "\t//                void *recvbuf, int recvcount, MPI_Datatype recvtype, int root,\n",
    "\t//                MPI_Comm comm);\n",
    "\tMPI_Gather(recbuf, SIZE, MPI_INT, fulbuf, SIZE, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "\tif(rank == 0) {\n",
    "\t\tprintf(\"\\n\");\n",
    "\t\tfor(i=0; i < numtasks; i++) {\n",
    "\t\t\tfor(j=0; j<SIZE; j++)\n",
    "\t\t\t\tprintf(\"%2d \",fulbuf[i*SIZE + j]);\n",
    "\t\t\tprintf(\"\\n\");\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\t\n",
    "\treturn(0);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aI7gBVC2Cuxx",
    "outputId": "371fe85c-c2f5-4a87-c47c-5518a34082fb"
   },
   "outputs": [],
   "source": [
    "# Aqui, testamos com 4 processos no mesmo nó. Se houver mais nós, basta configurá-los em um hostfile, ou especificar em linha de comando\n",
    "!mpicc -Wall scatter.c -o scatter && mpirun -n 4 -host localhost:4 scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5jTzfBreoZY"
   },
   "source": [
    "# MPI_Reduce\n",
    "\n",
    "As operações globais de redução ([MPI_Reduce](https://www.open-mpi.org/doc/v3.1/man3/MPI_Gather.3.php), MPI_Op_create, MPI_Op_free,  MPI_Allreduce, MPI_Reduce_scatter, MPI_Scan) realizam a **sumarização global** de algum valor provido por todos os processos de um grupo.\n",
    "\n",
    "A operação de sumarização pode ser alguma entre as operações pré-definidas (*), como a soma ou a multiplicação dos valores, a identificação do maior ou do menor valor, ou novas operações definidas pela aplicação .\n",
    "\n",
    "```\n",
    "(*) Name                Meaning\n",
    "   ---------           --------------------\n",
    "    MPI_MAX             maximum\n",
    "    MPI_MIN             minimum\n",
    "    MPI_SUM             sum\n",
    "    MPI_PROD            product\n",
    "    MPI_LAND            logical and\n",
    "    MPI_BAND            bit-wise and\n",
    "    MPI_LOR             logical or\n",
    "    MPI_BOR             bit-wise or\n",
    "    MPI_LXOR            logical xor\n",
    "    MPI_BXOR            bit-wise xor\n",
    "    MPI_MAXLOC          max value and location\n",
    "    MPI_MINLOC          min value and location\n",
    "```\n",
    "Enquanto a operação MPI_Reduce produz o valor da redução em apenas um processo definido, [MPI_Allreduce](https://www.open-mpi.org/doc/v3.1/man3/MPI_Allreduce.3.php) produz o mesmo resultado nos *buffers* de todos os processos do grupo.\n",
    "\n",
    "Já a chamada [MPI_Reduce_scatter](https://www.open-mpi.org/doc/v3.1/man3/MPI_Reduce_scatter.3.php) combina operação de redução com a distribuição, operando sobre um vetor de resultados combinados.\n",
    "\n",
    "```\n",
    "int MPI_Reduce(const void *sendbuf, void *recvbuf, int count,\n",
    "               MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm);\n",
    " \n",
    "int MPI_Ireduce(const void *sendbuf, void *recvbuf, int count,\n",
    "                MPI_Datatype datatype, MPI_Op op, int root,\n",
    "                MPI_Comm comm, MPI_Request *request);\n",
    "```\n",
    "\n",
    "Entre os campos desta chamada, como era de se esperar, aparecm o endereço do ***buffer*** **de envio**, que vale para todos os nós, a indicação do processo que será o ***root*** desta operação, ou seja o ***rank*** daquele que vai receber todos os dados e sumarizá-los, a operação que será aplicada para a redução, além do tipo e a contagem dos dados.\n",
    "\n",
    "Há algumas primitivas diferetes para redução, como descritas a seguir.\n",
    "\n",
    "* **MPI_Reduce**(&sendbuf,&recvbuf,count,datatype,op,root,comm)\n",
    "Applies a reduction operation on all tasks in the group and places the result in one task.\n",
    "* **MPI_Allreduce**(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm).  \n",
    "Same as MPI_Reduce except that the result appears in the receive buffer of all the group members. Applies a reduction operation and places the result in all tasks in the group. This is equivalent to an MPI_Reduce followed by an MPI_Bcast.\n",
    "* **MPI_Reduce_scatter**(&sendbuf,&recvbuf,recvcount,datatype, ...... op,comm). \n",
    "First does an element-wise reduction on a vector across all tasks in the group. Next, the result vector is split into disjoint segments and distributed across the tasks. This is equivalent to an MPI_Reduce followed by an MPI_Scatter operation.\n",
    "* **MPI_Scan**(&sendbuf,&recvbuf,count,datatype,op,comm). \n",
    "Performs a scan operation with respect to a reduction operation across a task group.\n",
    "\n",
    "Resumidamente, a chamada MPI_Reduce, aplica uma operação de redução sobre dados recebidos de todos os processos de um grupo.\n",
    "\n",
    "O exemplo a seguir ilustra o uso da primitiva MPI_Reduce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRq4RHq3etgh",
    "outputId": "ab0c18e5-bf0d-4066-9ccb-4eb9f6d514bb"
   },
   "outputs": [],
   "source": [
    "%%writefile reduce.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <unistd.h>\n",
    "#include <time.h>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "\tint rank, numtasks;\n",
    "  int num, redutor = 0;\n",
    "\n",
    "\tMPI_Init(&argc,&argv);\n",
    "\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
    "\n",
    "  // srand(time(NULL));\n",
    "  srand(getpid());\n",
    "  num = rand() % 100;\n",
    " \n",
    "\t// collective communications \n",
    "\n",
    "\t// int MPI_Reduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype,\n",
    "\t//                 MPI_Op op, int root, MPI_Comm comm);\n",
    "\tMPI_Reduce(&num, &redutor, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  // Apenas o processo de rank 0, neste caso, deve imprimir um valor consistente\n",
    "  // já que ele foi o 'root' desta operação\n",
    "\tprintf(\"Rank %d: num: %d  sum: %d\\n\", rank, num, redutor);\n",
    "\n",
    "\t// int MPI_Allreduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, \n",
    "\t//                    MPI_Op op, MPI_Comm comm )\n",
    "\tMPI_Allreduce(&num, &redutor, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n",
    "\n",
    "  // Todos os processos devem ter recebido uma cópia do valor gerado com a redução agora\n",
    "\tprintf(\"Rank %d: num: %d  max: %d\\n\", rank, num, redutor);\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_KNKHp-ew3W",
    "outputId": "7125d205-cde3-4146-df28-79e1a9ace44f"
   },
   "outputs": [],
   "source": [
    "! if [ ! reduce -nt reduce.c ]; then mpicc -Wall reduce.c -o reduce; fi && mpirun -n 4 -host localhost:4 reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb8edXRnOckg"
   },
   "source": [
    "Vejamos mais um exemplo de operação de redução.\n",
    "\n",
    "Como se pode ver nos parâmetros da primitiva MPI_Reduce, os *buffers* de envio e recebimento podem ser um vetor, já que os demais parâmetros indicam o tipo dos dados e o número de elementos. Assim, a operação de redução pode ser aplicada a cada um dos elementos deste vetor!\n",
    "\n",
    "Do ponto de vista de desempenho, contudo, talvez esse tipo de operação não seja muito eficiente. Embora a implementação da biblioteca MPI trate de todos os detalhes desta operação, é possível que uma solução mais eficiente seja obtida caso a redução seja feita localmente, por exemplo, usando algum algoritmo em *log_n* etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkA8KOQIjD9X",
    "outputId": "0b8908d3-52ad-46e0-87b3-1efd29ed4396"
   },
   "outputs": [],
   "source": [
    "%%writefile red-vet.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <unistd.h>\n",
    "#include <time.h>\n",
    "#include <mpi.h>\n",
    "#include <string.h>\n",
    "\n",
    "#define NELEM 10\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "\tint i, rank, numtasks;\n",
    "  int num[NELEM], redutor[NELEM];\n",
    "\n",
    "\tMPI_Init(&argc,&argv);\n",
    "\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
    "\n",
    "  // srand(time(NULL));\n",
    "  srand(getpid());\n",
    " \n",
    "  // cada nó gera seus valores locais\n",
    "  for (i=0; i < NELEM; i++)\n",
    "    num[i] = rand() % 10;\n",
    " \n",
    "  memset(redutor, 0, NELEM* sizeof(int));\n",
    " \n",
    "  printf(\"%d: \", rank);\n",
    "  for(i=0; i < NELEM; i++)\n",
    "    printf(\"%d \",num[i]);\n",
    "  printf(\"\\n\"); \n",
    " \n",
    "\t// collective communications \n",
    "\n",
    "\t// int MPI_Reduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype,\n",
    "\t//                 MPI_Op op, int root, MPI_Comm comm);\n",
    "\tMPI_Reduce(num, redutor, NELEM, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  // Apenas o processo de rank 0, neste caso, deve imprimir um valor consistente\n",
    "  // já que ele foi o 'root' desta operação\n",
    "  printf(\"%d: \", rank);\n",
    "\tfor (i=0; i < NELEM; i++)\n",
    "    printf(\"%d \", redutor[i]);\n",
    "  printf(\"\\n\");\n",
    " \n",
    "\t// int MPI_Allreduce (void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype, \n",
    "\t//                    MPI_Op op, MPI_Comm comm )\n",
    "\tMPI_Allreduce (num, redutor, NELEM, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n",
    "\n",
    "  // Todos devem ter os valores da redução agora\n",
    "  printf(\"%d: \", rank);\n",
    "\tfor (i=0; i < NELEM; i++)\n",
    "    printf(\"%d \", redutor[i]);\n",
    "  printf(\"\\n\");\n",
    " \n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlllqJJFjOAo"
   },
   "outputs": [],
   "source": [
    "! if [ ! red-vet -nt red-vet.c ]; then mpicc -Wall red-vet.c -o red-vet; fi && mpirun -n 4 -host localhost:4 red-vet"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Gb9WsW6XVBeC",
    "nhDygKaQe2pH"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
