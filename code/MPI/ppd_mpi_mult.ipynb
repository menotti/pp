{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu7CnSM6WTQa"
   },
   "source": [
    "# PPD: Programação Paralela e Distribuída\n",
    "\n",
    "Hélio Crestana Guardia - DC / UFSCar - 2023\n",
    "\n",
    "**Programa**: multiplicação de matrizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoUr5VSI1a3t"
   },
   "source": [
    "De que maneira o programa da multiplicação de matrizes poderia ser paralelizado com MPI? \n",
    "\n",
    "O programa a seguir aprsenta o esqueleto da uma solução para este problema. Como estamos tratando de computação paralela usando computadores distintos, sem áreas de memória compartilhada para as comunicações, precisamos tratar explicitamente de toda comunicação e sincronização entre os processos executando nos diversos nós.\n",
    "\n",
    "O probleme começa com a decisão de como será feito o particionamento e de quais dados precisam ser passados a quais nós para as manipulações.\n",
    "\n",
    "Algumas questões a considerar neste problema:\n",
    "\n",
    "* Quem lê os dados das matrizes ou gera os valores dos elementos? rank 0?\n",
    "* Como dividir os cálculos entre os diversos processos? Cada um calcula um grupo de linhas da matriz C?\n",
    "* Como informar a cada processo quantas (e quais?) linhas (ou conjuntos de elementos) ele irá calcular?\n",
    "* Supondo a divisão do trabalho com o cálculo de linhas, o que o nó de rank 0 precisa enviar para cada nó? Matriz A inteira e apenas as linhas específicas de B, ou a matriz B é enviada inteira a todos?\n",
    "* E possível usar MPI_Scatter para enviar a matriz B aos demais processos? Será que se o número de elementos da matriz B não for múltiplo do número de processos, a operação de Scatter poderia gerar erros na distribuição das linhas?\n",
    "* É possível usar MPI_Gather para receber os dados da matriz C calculados pelos nós? \n",
    "* É possível usar MPI_Bcast para enviar a todos? O que seria enviado desta forma?\n",
    "* O nó de rank 0 deve realizar cálculos ou atua apenas como coordenador?\n",
    "* Como receber os resultados (linhas de C) dos nós? As mensagens podem chegar fora de ordem?\n",
    "* É necessária alguma sincronização?\n",
    "\n",
    "Como o modelo de execução de MPI é comumente SPMD, o mesmo código é executado por todos os processos. Como se vê no programa a seguir, contudo, é possível diferenciar atividades dentro do mesmo código, comumente em função dos *ranks* dos processos, para que processos distintos realizem operações distintas.\n",
    "\n",
    "Usando o Colab mesmo, é possível editar essa versão do programa e executar aqui.\n",
    "\n",
    "O exemplo de execução apresentado depois do código realiza a execução e a medição do tempo de execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1uIiQ8hDTDh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kD9l8frY6yA0",
    "outputId": "9b3ca422-1480-466a-8db0-c9cefd751fc8"
   },
   "outputs": [],
   "source": [
    "%%writefile mult.c\n",
    "\n",
    "/* \n",
    "** Universidade Federal de Sao Carlos\n",
    "** PPD: Programação Paralela e Distribuída\n",
    "** Hélio Crestana Guardia\n",
    "*/\n",
    "\n",
    "/*\n",
    "** Programa : multiplicacao de matrizes\n",
    "** Objetivo: paralelizacao com MPI\n",
    "*/\n",
    "\n",
    "#include <math.h> \n",
    "#include <stdlib.h> \n",
    "#include <string.h> \n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "#include <time.h>\n",
    "#include \"mpi.h\"\n",
    "\n",
    "float *A, *B, *C;\n",
    "\n",
    "// Informações a serem enviadas pelo rank 0 aos demais processos\n",
    "struct info {\n",
    "  int lin_a;    // número de linhas da matriz A \n",
    "  int col_a;    // número de colunas da matriz A (= lin_b)\n",
    "  int lin_b;    // número de linhas da matriz B (= col_a)\n",
    "  int col_b;    // número de colunas da matriz B\n",
    "  int inic;     // número da linha inicial que o processo irá calcular\n",
    "  int numlin;   // número de linhas que serão calculadas pelo processo\n",
    "} s_info;\n",
    "\n",
    "int\n",
    "main(int argc, char *argv[])\n",
    "{\n",
    "\tint lin_a,col_a,lin_b,col_b,lin_c,col_c;\n",
    "\tint i,j,k, t;\n",
    "\tint numtasks, rank;\n",
    "\tint result, sum;\n",
    "\tint numlin, resto;\n",
    "\n",
    "\t// Todos os processos iniciam a biblioteca e determinam seus ranks na aplicação\n",
    " \n",
    "\tresult = MPI_Init(&argc,&argv);\n",
    "\tif (result != MPI_SUCCESS) {\n",
    "\t\tfprintf(stderr,\"Erro iniciando MPI: %d\\n\",result);\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
    "\t}\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "\n",
    "/* \n",
    "  Atividades do processo com rank 0:\n",
    "  ---------------------------------\n",
    "  - determinar dimensões das matrizes\n",
    "  - alocar espaço e carregar os dados das matrizes na memória local\n",
    "  - determinar como será a divisão do trabalho entre os processos\n",
    "  - enviar a cada processo as informações sobre as matrizes e sobre os cálculos que este irá realizar\n",
    "  - enviar a cada processo as informações das matrizes pertinentes para os cálculos\n",
    "  - receber resultados dos cálculos\n",
    " \n",
    "  Atividades dos processos com rank > 0:\n",
    "  ---------------------------------\n",
    "  - Aguardar (e receber) informações sobre as matrizes e a divisão do trabalho\n",
    "\t- Alocar espaço em memória para armazenar os dados das matrizes que irão receber\n",
    "  - Receber os dados pertinentes das matrizes, posicionando-os em memória para os cálculos\n",
    "  - Realizar os cálculos locais\n",
    "  - Enviar os valores processados de volta ao processo de rank 0\n",
    "*/\n",
    "  \n",
    "\tif(rank==0) {\n",
    "\n",
    "    /********** Atividades do processo com rank 0 *************************/\n",
    "\n",
    "    // Nó com rank 0 faz a leitura das dimensõe das matrizes\n",
    "\n",
    "\t\tsetbuf(stdout,NULL); // para forçar a exibição imediata dos textos no terminal\n",
    "\n",
    "\t\tprintf(\"Linhas A: \");\n",
    "\t\tscanf(\"%d\",&lin_a);\n",
    "\t\tprintf(\"Colunas A / Linhas B: \");\n",
    "\t\tscanf(\"%d\",&col_a);\n",
    "\t\tlin_b = col_a;\n",
    "\t\tprintf(\"Colunas B: \");\n",
    "\t\tscanf(\"%d\",&col_b);\n",
    "\t\tprintf(\"\\n\");\n",
    "\t\tlin_c = lin_a;\n",
    "\t\tcol_c = col_b;\n",
    " \n",
    "  \t// Nó rank 0 aloca espaço para as matrizes e as preenche de forma aleatória.\n",
    "    // Numa aplicação efetiva, provavelmente leria dados de arquivos\n",
    " \n",
    "\t\t// Alocacao dinâmica das matrizes, com linhas em sequência \n",
    "\t\tA=(float *)malloc(lin_a*col_a*sizeof(float));\n",
    "\t\tB=(float *)malloc(lin_b*col_b*sizeof(float));\n",
    "\t\tC=(float *)malloc(lin_c*col_c*sizeof(float));\n",
    " \n",
    "\t\t// Inicia gerador de números aleatórios. Comentar comando a seguir se quiser\n",
    "\t\t// gerar sempre os mesmos valores para uniformidade nos cálculos.\n",
    "\t\tsrandom(time(NULL));\n",
    "\n",
    "\t\tfor(i=0; i < lin_a * col_a; i++) \n",
    "\t\t\tA[i]=(float)rand() / (float)RAND_MAX; \n",
    "\n",
    "\t\tfor(i=0; i < lin_b * col_b; i++) \n",
    "\t\t\tB[i]=(float)rand() / (float)RAND_MAX; \n",
    "\t\n",
    "\t  // Envio das matrizes para os processos. \n",
    "    // O que enviar depende de como os cálculos serão divididos.\n",
    "    // Aqui, considerando divisão das linhas de C\n",
    " \n",
    "\t\t// preenche informações sobre as matrizes\n",
    "\t\ts_info.lin_a = lin_a;\n",
    "\t\ts_info.col_a = col_a;\n",
    "\t\ts_info.lin_b = lin_b;\n",
    "\t\ts_info.col_b = col_b;\n",
    "\n",
    "\t\t// informações sobre a linha inicial e o número de linhas dependem do rank\n",
    "\n",
    "\t\t// Determina quantas e quais linhas cada processo (rank) vai calcular\n",
    "\t\t// rank 0 não irá participar dos cálculos...\n",
    "\n",
    "\t\tnumlin = lin_a / (numtasks -1);\n",
    "\t\tresto = lin_a % (numtasks -1);\n",
    "\n",
    "\t\t// Determina linha inicial e número de linhas para cada processo rank > 0\n",
    "    // e lhe envia linhas apropriadas\n",
    "\n",
    "\t\tfor(t=1; t < numtasks; t++) {\n",
    "      \n",
    "\t\t\t// Determina informações sobre linhas a calcular pelo processo\n",
    "\t\t\ts_info.numlin = numlin;\n",
    "\t\t\tif(t <= resto)\n",
    "\t\t\t\ts_info.numlin += 1;  // resto primeiros processos recebem 1 linha a mais\n",
    "\t\t\ts_info.inic = (t-1) * numlin;\n",
    "\t\t\tif(resto) {\n",
    "\t\t\t\tif(t<=resto)\n",
    "\t\t\t\t\ts_info.inic += t-1;    // resto primeiros processos recebem 1 linha a mais\n",
    "\t\t\t\telse\n",
    "\t\t\t\t\ts_info.inic += resto;   // resto primeiros processos recebem 1 linha a mais\n",
    "\t\t\t}\n",
    "      // Envia informações de controle para demais processos rank > 0\n",
    "\n",
    "\t\t\t// MPI_Send( &s_info, sizeof(s_info), MPI_INT, t, 1, MPI_COMM_WORLD);\n",
    "\t\t\tMPI_Send( &s_info, sizeof(s_info), MPI_CHAR, t, 1, MPI_COMM_WORLD);\n",
    "\n",
    "\t\t\t// Envia linhas de A aos processos rank = t (>0)\n",
    "\t\t\t// Enviar 1 linha de cada vez ou todas em sequência?\n",
    "      // Como linhas estão contíguas na memória, poderia usar MPI_Scatter?\n",
    "\t\t\t// Fazer o recebimento correspondende nos demais ranks\n",
    "\n",
    "\t\t\tfor (i=s_info.inic; i < s_info.inic + s_info.numlin; i++)\n",
    "\t\t\t\tMPI_Send( &A[i*lin_a], col_a, MPI_INT, t, 1, MPI_COMM_WORLD);\n",
    "    }\n",
    "  \n",
    "  \t// Nessa estratégia de particionamento, todos precisam da matriz B inteira. \n",
    "\t \t// Como enviá-la, replicando ou em Bcast? \n",
    "\t  // Bcast da matriz inteira ou linha por linha?\n",
    "\t\t// Observar que operação de Bcast, coletiva, deve ser realizada por todos os processos\n",
    "\t  for (i=0; i < lin_b; i++) \n",
    "\t\t  MPI_Bcast (&B[i*col_b], col_b, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "    // Recebe resultados finais. Rank 0 recebe linhas de C\n",
    "\t  // É possível receber as linhas de C fora de ordem?\n",
    " \n",
    "  \tfor(t=1; t < numtasks; t++) {\n",
    "        \n",
    "\t  \t// Determina informações sobre linhas que foram calculadas por cada processo rank > 0 \n",
    "\t\t \t// Poderia ter salvo essas infos, já calculadas no envio, num vetor de parâmetros...\n",
    "\n",
    "\t  \ts_info.numlin = numlin;\n",
    "\t  \tif(t <= resto)\n",
    "\t  \t\ts_info.numlin += 1;  // resto primeiros processos recebem 1 linha a mais\n",
    "\t  \ts_info.inic = (t-1) * numlin;\n",
    "\t  \tif(resto) {\n",
    "\t  \t\tif(t<=resto)\n",
    "\t  \t\t\ts_info.inic += t-1;    // resto primeiros processos recebem 1 linha a mais\n",
    "\t  \t\telse\n",
    "\t  \t\t\ts_info.inic += resto;   // resto primeiros processos recebem 1 linha a mais\n",
    "\t  \t}\n",
    "\n",
    "\t  \t// Recebe as linhas de C calculadas em cada processo rank > 0\n",
    "      // Podeira fazer o recebimento fora de ordem?\n",
    "      // Ideia: usar MPI_ANY_SOURCE... requer recebimento em buffer e cópia para\n",
    "      // posição efetiva...\n",
    "\t  \tfor (i=s_info.inic; i < s_info.inic + s_info.numlin; i++)\n",
    "\t  \t\tMPI_Recv( &C[i*col_c], col_c, MPI_INT, t, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    }\n",
    "\n",
    "\t} else { \n",
    "\n",
    "    /********** Atividades dos processos com rank > 0 *************************/\n",
    "\n",
    "  \t// Recebimento das matrizes e parâmetros para os cálculos\n",
    "                   \n",
    "\t\t// Recebem informações sobre as matrizes\n",
    "\t\t// MPI_Recv (&s_info, sizeof(s_info), MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "\t\tMPI_Recv (&s_info, sizeof(s_info), MPI_CHAR, 0, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "\n",
    "\t\n",
    "\t\tprintf(\"%d recebeu: A[%d,%d], B[%d,%d], inic: %d, numlin: %d\\n\",\n",
    "\t\t\trank, s_info.lin_a, s_info.col_a, s_info.lin_b, s_info.col_b, s_info.inic, s_info.numlin);\n",
    "\t\n",
    "\t\t// Alocacam espaços para as matrizes. \n",
    "\t\t// Alocam todo o espaço ou apenas para conter as linhas que irão manipular?\n",
    "\t\t// Matrizes A e C precisam de espaço apenas para as linhas que serão manipuladas pelo processo \n",
    "  \n",
    "\t\t// Alocacao dinâmica das matrizes, com linhas em sequência \n",
    "\t\tB = (float *)malloc( s_info.lin_b * s_info.col_b * sizeof(float));\n",
    "\t\t// A = (float *)malloc( s_info.lin_a * s_info.col_a * sizeof(float));\n",
    "\t\tA = (float *)malloc( s_info.numlin * s_info.col_a * sizeof(float));\n",
    "\t\t// C = (float *)malloc( s_info.lin_a * s_info.col_b * sizeof(float));\n",
    "\t\tC = (float *)malloc( s_info.numlin * s_info.col_b * sizeof(float));\n",
    " \n",
    "\t\t// Recebe linhas de A (s_info.numlin), posicionando-as na matriz alocada\n",
    "\t\tfor (i=0; i < s_info.numlin; i++)     \n",
    "\t\t\tMPI_Recv( &A[i*s_info.col_a], s_info.col_a, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "  \n",
    "\t\t// Recebe as linhas de B enviadas em Bcast\n",
    "\t\tfor (i=0; i < s_info.lin_b; i++) \n",
    "\t\t\tMPI_Bcast (&B[i*s_info.col_b], s_info.col_b, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  \t// cálculo da multiplicacao, feito pelos processos de rank > 0\n",
    "  \n",
    "\t\t// Cada processo calcula s_info.numlin linhas\n",
    "\n",
    "\t\tfor ( i=0; i < s_info.numlin; i++) \n",
    "\t\t\tfor ( j=0; j < s_info.col_b; j++) {\n",
    "\t\t\t\t// C[ i * s_info.col_b +j ] = 0; \n",
    "\t\t\t\tsum = 0;   \n",
    "\t\t\t\tfor ( k=0; k < s_info.col_a; k++) \n",
    "\t\t\t\t\t// C[ i * s_info.col_b +j ] += A[ i * s_info.col_a +k ] * B[ k * s_info.col_b +j ];\n",
    "\t\t\t\t\tsum += A[ i * s_info.col_a +k ] * B[ k * s_info.col_b +j ];\n",
    "\t\t\t\tC[ i * s_info.col_b +j ] = sum;\n",
    "\t\t\t}\n",
    "\t    \n",
    "\t\t// Envia linhas da matriz C calculadas localmente ao process rank 0\n",
    "\n",
    "\t\tfor (i=0; i < s_info.numlin; i++)\n",
    "\t\t\tMPI_Send( &C[i*s_info.lin_a], s_info.col_b, MPI_INT, 0, 1, MPI_COMM_WORLD);\n",
    "\t}\n",
    "\n",
    "/* É possível comparar os resultados calculados paralelamente com os resultados\n",
    " * calculados de forma sequencial?\n",
    " * Hum... se os dados forem float, a ordem das operações pode gerar resultados\n",
    " * diferentes...\n",
    " */\n",
    "\n",
    "  // Se quiser testar os resultados produzidos de forma paralela  \n",
    "// #define DEBUG\n",
    " \n",
    "#ifdef DEBUG\n",
    "  if(rank==0) {\n",
    "   // Cálculo sequencial para comparações\n",
    "  \tfloat *AB=(float *)malloc(lin_c*col_c*sizeof(float));\n",
    "  \tfor(i=0; i < lin_c; i++) \n",
    "  \t\tfor(j=0; j < col_c; j++) {\n",
    "  \t\t\t// AB[i*col_c+j]=0;    \n",
    "\t\t\t\tsum = 0;\n",
    "  \t\t\tfor(k=0; k < col_a; k++) \n",
    "  \t\t\t\t// AB[i*col_c+j] = AB[i*col_c+j] + A[i*col_a+k] * B[k*col_b+j];\n",
    "  \t\t\t\tsum += A[i*col_a+k] * B[k*col_b+j];\n",
    "\t\t\t\tAB[i*col_c+j] = sum;\n",
    "  \t\t}\n",
    "   // Comparação da matriz calculada sequencialmente com a matriz calculada em paralelo\n",
    "  \tfor(i=0;i<lin_c;i++)\n",
    "  \t\tfor(j=0;j<col_c;j++)\n",
    "  \t\t\tif(C[i*col_c+j] != AB[i*col_c+j])\n",
    "  \t\t\t\tprintf(\"Erro em %d,%d\\n\",i,j);\n",
    "  }\n",
    "#endif\n",
    " \n",
    "\t// Todos os processos\n",
    " \n",
    "  // Libera áreas de memória\n",
    "  free(A); free(B); free(C);\n",
    " \n",
    "\tMPI_Finalize();\n",
    " \n",
    "\treturn(0);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mtkY88VV3I_",
    "outputId": "4cb7c3d2-4ed3-49fb-e250-8fe74ffb472f"
   },
   "outputs": [],
   "source": [
    "%%writefile dados\n",
    "1024\n",
    "1024\n",
    "1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkMXaSafS7l2",
    "outputId": "5bd307a4-e21c-467c-b000-adf11f989c1d"
   },
   "outputs": [],
   "source": [
    "! if [ ! mult -nt mult.c ]; then mpicc mult.c -o mult -O3 ; fi\n",
    "# Execução com medição de tempo pelo comando time. Observe o tempo total decorrido (real)\n",
    "! time mpirun -n 5 ./mult < dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
