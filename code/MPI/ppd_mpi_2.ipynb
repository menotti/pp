{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_vqFo-wFgJn"
   },
   "source": [
    "# PPD: MPI e programação com passagem de mensagem\n",
    "\n",
    "Hélio - DC/UFSCar - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqLeez1BbscH"
   },
   "source": [
    "# Aspectos gerais das transmissões\n",
    "\n",
    "Usando o mecanismo de **identificação lógica** (***rank***) dos processos emissores (***senders***) e receptores (***receivers***), MPI oferece suporte tanto para comunicações **diretas** (ponto-a-ponto) entre pares de processos, quanto para operações de comunicação em **grupo** (coletivas). \n",
    "\n",
    "Relembrando, as identificações lógicas levam em consideração a participação de todos os processos no grupo MPI_COMM_WORLD, variando de 0 a N-1. \n",
    "\n",
    "    Também é possível criar outros sub-grupos numa aplicação. Dentro de um sub-grupo, \n",
    "    cada processo também vai ter um identificador lógico, que pode ser usado em comunicações \n",
    "    entre membros deste sub-grupo.\n",
    "\n",
    "Nas transmissões ponto-a-ponto, especifica-se um grupo de processos, que geralmente é o grupo MPI_COMM_WORLD, formado por todos os processos dessa aplicação. Os identificadores lógicos (*ranks*) dos processos **neste grupo** são usados então para identificar o emissor e o receptor de cada operação. \n",
    "\n",
    "Ex:\n",
    "\n",
    "* int MPI_**Send** (void \\*buf, int count, MPI_Datatype dtype, \n",
    "  int **dest**, int tag, MPI_Comm **comm**)\n",
    "\t\t\n",
    "* int MPI_**Recv** (void \\*buf, int count, MPI_Datatype dtype, \n",
    "  int **src**, int tag, MPI_Comm **comm**, MPI_Status *stat)\n",
    "\n",
    "<br>\n",
    "\n",
    "Cada mensagem, por sua vez, possui um atributo de identificação (***Tag***), que pode ser usado na seleção de mensagens a receber. Nas operações de recebimento de mensagens, é possível especificar um processo de origem e um identificador de mensagem esperada (*Tag*). Há, contudo, alguns códigos especiais que podem ser usados para identificar **qualquer processo** origem (**MPI_ANY_SOURCE**) e **qualquer mensagem** (**MPI_ANY_TAG**).\n",
    "\n",
    "Ex:\n",
    "\n",
    "* int MPI_**Send** (void \\*buf, int count, MPI_Datatype dtype,\n",
    "  int **dest**, int **tag**, MPI_Comm **comm**)\n",
    "\t\t\n",
    "* int MPI_**Recv** (void \\*buf, int count, MPI_Datatype dtype, \n",
    "  int **src**, int **tag**, MPI_Comm **comm**, MPI_Status *stat)\n",
    "\n",
    "<br>\n",
    "\n",
    "Já nas transmissões **coletivas**, identifica-se o **grupo**, ou sub-grupo, envolvido, e o processo que fará o papel de **divulgador** ou **agregador** dos dados, dependendo do tipo de transmissão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WFEXah-Phba"
   },
   "source": [
    "# Tipos dos dados transmitidos\n",
    "\n",
    "A transmissão de dados usando *sockets* é feita passando-se um ponteiro para uma sequência de bytes. Cabe à aplicação tratar dos detalhes do armazenamento e da leitura dos dados, de acordo com seus tipos.\n",
    "\n",
    "MPI simplifica o envio de sequências de dados de um mesmo tipo. Considerando os dados transmitidos nas mensagens, MPI permite que a aplicação identifique seus **tipos** e **quantidades**, cuidando automaticamente dos empacotamentos apropriados em cada caso. Também é possível fazer empacotamentos e desempacotamentos de conteúdos específicos. \n",
    "\n",
    "\n",
    "Os tipos dos dados transmitidos podem ser pré-definidos ou definidos pelo usuário.\n",
    "\n",
    "* MPI_CHAR: signed char\n",
    "* MPI_SHORT: signed short int\n",
    "* MPI_INT: signed int\n",
    "* MPI_LONG: signed long int\n",
    "* MPI_UNSIGNED_CHAR: unsigned char\n",
    "* MPI_UNSIGNED_SHORT: unsigned short int\n",
    "* MPI_UNSIGNED: unsigned int\n",
    "* MPI_UNSIGNED_LONG: unsigned long int\n",
    "* MPI_FLOAT: float\n",
    "* MPI_DOUBLE: double\n",
    "* MPI_LONG_DOUBLE: long double\n",
    "* MPI_BYTE: 8 binary digits\n",
    "* MPI_PACKED: data packed or unpacked with MPI_Pack()/ MPI_Unpack\n",
    "\n",
    "Ex:\n",
    "\n",
    "* int MPI_**Send** (void \\*buf, int **count**, MPI_Datatype **dtype**, \n",
    "  int dest, int tag, MPI_Comm comm)\n",
    "\t\t\n",
    "* int MPI_**Recv** (void \\*buf, int **count**, MPI_Datatype **dtype**, \n",
    "  int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
    "\n",
    "Nas operações acima, a indicação do tipo dos dados transmitidos e o número de dados serve para que MPI faça o empacotamento e desempacotamento de maneira apropriada. Assim, o conteúdo dos dados transmitidos é preservado nas transmisões, mesmo entre sistemas emissor e receptor executando em SOs e arquiteturas diferentes.\n",
    "\n",
    "<br>\n",
    "\n",
    "Também é possível realizar o envio de dados de tipos variados. Para isso, MPI oferece mecanismos para o empacotamento de dados em um *buffer* para envio e o desempacotamento de dados de uma mensagem recebida.\n",
    "\n",
    "É claro que cabe ao código da aplicação fazer o empacotamento e o desempacotamento dos dados no buffer na ordem apropriada.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Emapacotamento e envio de dados variados\n",
    "\n",
    "https://www.open-mpi.org/doc/v3.1/man3/MPI_Pack.3.php\n",
    "<br>\n",
    "https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report/node92.htm#Node92\n",
    "\n",
    "\n",
    "```\n",
    "int MPI_Pack (const void *inbuf, int incount, MPI_Datatype datatype,\n",
    "              void *outbuf, int outsize, int *position, MPI_Comm comm)\n",
    "```\n",
    "\n",
    "```\n",
    " Example: An example using MPI_Pack:\n",
    "\n",
    "    int position, i, j, a[2];\n",
    "    char buff[1000];\n",
    "    ....\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n",
    "    if (myrank == 0)\n",
    "    {\n",
    "       / * SENDER CODE */\n",
    "    position = 0;\n",
    "      MPI_Pack(&i, 1, MPI_INT, buff, 1000, &position, MPI_COMM_WORLD);\n",
    "      MPI_Pack(&j, 1, MPI_INT, buff, 1000, &position, MPI_COMM_WORLD);\n",
    "      MPI_Send( buff, position, MPI_PACKED, 1, 0, MPI_COMM_WORLD);\n",
    "    }\n",
    "    else  /* RECEIVER CODE */\n",
    "      MPI_Recv( a, 2, MPI_INT, 0, 0, MPI_COMM_WORLD)\n",
    "    }\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYIqCr2zR-v2"
   },
   "source": [
    "## Tipos de transmissão ponto-a-ponto\n",
    "\n",
    "Há diferentes tipos de primitivas de envio ponto-a-ponto, que variam em função do **sincronismo** entre emissor e receptor, do **bloqueio** ou não da operação, e do uso de ***buffers*** no transmissor e no receptor.\n",
    "\n",
    "* ***Standard***: nas transmissões padrão, não há sincronismo entre emissor e receptor. Se forem providos buffers, envio pode ser concluído antes do recebimento. No caso de transmissões não-bloqueantes, as operações MPI_Wait( ) e MPI_Test( ) podem ser usadas para saber se foram concluídas.\n",
    "* ***Buffered***: transmissões podem ser *bufferizadas*. Para tanto, as chamadas MPI_Buffer_attach( ) e MPI_Buffer_detach( ) tratam da definição de espaços de *buffer*. Nesse tipo de transmissão bufferizada, o envio pode ser concluído antes do recebimento ser selecionado.\n",
    "* ***Synchronous***: nas transmissões síncronas, as operações envio e recebimento podem ser realizadas em qualquer ordem, mas a transmissão só ocorre quando ambas as operações forem emitidas. Deste modo, além de prover a transmissão, essas chamadas servem para a sincronziação entre as partes envolvidas.\n",
    "* ***Ready***: nesse modo de transmissão, o envio pode ser iniciado apenas quando o recebimento já foi solicitado, ou um erro é resultado.\n",
    "\n",
    "<br>\n",
    "\n",
    "    Nas chamadas providas pela API MPI, mnemônicos nos nomes das funções especificam cada um desses modos de transmissão: \n",
    "    -b: buffered, -s: synchronous, -r: ready\n",
    "\n",
    "Qualquer tipo de envio (padrão, buferizado, síncrono ou *ready*) pode ser associado a qualquer tipo de recepção (padrão, ...). \n",
    "\n",
    "Nas operações não bloqueantes, há formas de verificar posteriormente se mensagens esperadas já foram recebidas. \n",
    "\n",
    "Já as operações que usam *buffers* têm o efeito de permitir que as transmissões ocorram mesmo que emissor e receptor não estejam sincronizados numa operação de transmissão.\n",
    "\n",
    "Uma vez que as chamadas de envio e recebimento comumente podem ser emitidas de forma não sincronizada, **cabe à implementação MPI** tratar do posicionamento dos dados até que eles possam ser efetivamente repassados ao processo receptor.\n",
    "\n",
    "Por exemplo, caso a operação de envio seja realizada antes de o receptor emitir uma chamada de recepção, cabe à implementação MPI decidir onde os dados transmitidos serão armazenados até que possam ser entregues. Para tanto, uma área de *buffer* de recebimento deve ser alocada, seja no nó emissor e/ou no receptor. A decisão de como tratar isso, contudo, não é padronizada, sendo dependente da implementação MPI.\n",
    "\n",
    "Vale ressaltar que as transmissões providas pela biblioteca MPI **são confiáveis**. Ou seja, salvo se ocorra falha nos meios de transmissão, as mensagens enviadas serão sempre recebidas corretamente e a aplicação não precisa preocupar-se com a verificação de erros nos dados recebidos, com limites de tempos para transmissão (*time-outs*), ou com outras condições de erro. \n",
    "\n",
    "MPI garante ainda que a ordem de recebimento das mensagens equivalentes é respeitada nas entregas aos receptores, mas não cabe à implementação MPI garantir que não ocorrerá *starvation* no recebimento de mensagens por processos concorrentes. Ou seja, se vários processos competem pelo recebimento de algum tipo de mensagem, MPI não garante que todos receberão mensagens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb9WsW6XVBeC"
   },
   "source": [
    "# Comunicação com MPI_Send e MPI_Receive\n",
    "\n",
    "O exemplo a seguir ilustra transmissões usando [MPI_Send](https://www.open-mpi.org/doc/v3.1/man3/MPI_Send.3.php) e [MPI_Recv](https://www.open-mpi.org/doc/v3.1/man3/MPI_Recv.3.php).\n",
    "\n",
    "Uma vez compilado um programa MPI, ele pode ser ativado com diferentes números de processos, alocados sobre diferentes conjuntos de computadores. Além disso, comumente, o mesmo código (arquivo executável) é iniciado em todos os nós, no modelo SPMD.\n",
    "\n",
    "Deste modo, um aspecto comum na maior parte dos programas é determinar quantos processos foram usados na execução corrente e qual é o número lógico (*rank*) de um processo dentro deste conjunto. \n",
    "\n",
    "Isso é feito com as chamadas MPI_Comm_size(MPI_COMM_WORLD, ...) e MPI_Comm_rank(MPI_COMM_WORLD, ...);\n",
    "\n",
    "A diferenciação do papel que cada processo exeutará dentro da aplicação é comumente feita em função de seus *ranks*. Em geral, o processo de *rank* **0** é usado para fazer as atividades de coordenação, mas isso é critério da aplicação.\n",
    "\n",
    "```\n",
    "// Determina o número de processos e o rank do processo atual no grupo geral\n",
    "int num_procs, rank;\n",
    "\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "int i;\n",
    "int val;    // vai ser usado para transmissão pelo rank 0 e para recepção pelos demais processos\n",
    "...\n",
    "if (rank == 0) {\n",
    "    val = set_val();\n",
    "    // envia mensagem para todos os demais processos, 1 a rank-1\n",
    "    // int MPI_Send(const void *buf, int count, MPI_Datatype datatype, \n",
    "    //              int dest, int tag, MPI_Comm comm)\n",
    "    for(i=1; i < num_procs; i++)\n",
    "      MPI_Send(&val, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n",
    "\n",
    "} else if (rank != 0) { // processos rank > 0 recebem mensagem de rank 0\n",
    "    // int MPI_Recv(void *buf, int count, MPI_Datatype datatype, \n",
    "    //              int source, int tag, MPI_Comm comm, MPI_Status *status)\n",
    "    MPI_Recv(&val, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    printf(\"Processo %d recebeu valor %d do processo rank 0\\n\", rank, val); \n",
    "}\n",
    "```\n",
    "\n",
    "Alguns aspectos a notar neste trecho de código:\n",
    "\n",
    "* o **mesmo programa** está sendo executado nos processos de todos os *ranks*\n",
    "* no envio, veja que o buffer de transmissão é o endereço de onde se iniciam os dados. Neste caso, é o endereço da variável;\n",
    "* no envio ainda, o contador é o número de elementos, do tipo definido, que serão copiados para transmissão a partir do endereço de início do *buffer*;\n",
    "* no envio, o valor de ***i*** está sendo usado para indicar o *ranK* de cada receptor a que se destina a mensagem; \n",
    "* veja que, com o nó de *rank* ***0*** está transmitindo, o ***for*** varia de ***i=1 a (num_procs-1)***. Ou seja, 0 envia para os demais;\n",
    "* nessas transmissões, todas as mensagens têm o ***tag*** 0, tanto no envio quanto no recebimento; assim, não há seleção de mensagens pelos *tags* neste caso;\n",
    "* no recebimento, passa-se o endereço de onde os dados serão colocados, o tipo dos dados e o número de ocorrências;\n",
    "* no recebimento, todos os nós recebem do nó de *rank* ***0***;\n",
    "* ainda no recebimento, veja que o valor inteiro sendo recebido é copiado para a variável ***val***. É claro que isso não vai sobrepor o valor da variável val do emissor, já que essa operação de recebimento vai estar sendo executada só nos nós receptores (*rank* > 0)!\n",
    "\n",
    "Um outro aspecto a ressaltar aqui e que talvez esteja confuso, é o fato que embora estejamos examinando um código único, vai haver várias instâncias de processos executando esse mesmo código! \n",
    "\n",
    "Em uma das cópias, no nó de *rank* ***0***, aquele de iniciou a execução, vai ser executada a primeira parte do ***if***. Nos demais, o código do ***else***.\n",
    "\n",
    "Faz sentido?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4kAk0w6SW1P"
   },
   "source": [
    "# Transmissões não bloqueantes\n",
    "\n",
    "Em MPI, e na programação com passagem de mensagem de maneira geral,  as transmissões de mensagens podem também servir para algum tipo de **sincronização** entre os processos. O uso de *buffers*, explicitamente pela aplicação, ou pela biblioteca MPI, introduz aind um outro aspecto às transmissões. \n",
    "\n",
    "Comumente, a bibliotevca MPI utiliza *buffers* para transmissão, alocados no espaço de endereçamento do processo, mas de forma **não visível pela aplicação**. Assim, quando há uma chamada ***MPI_Send***, os dados a serem transmitidos são copiados para um *buffer* da biblioteca MPI e a chamada retorna imediatamente. Caso não haja espaço nesse *buffer*, devido a transmissões anteriores ainda pendentes, a tarefa que emitiu a chamada é bloqueada. \n",
    "\n",
    "Já quando a primitiva ***MPI_Isend*** for usada, a chamada retorna imediatamente, mesmo que os dados a transmitir não tenham sido copiados para o *buffer* da biblioteca. Esse é um comportamento **não bloqueante**. Um parâmetro extra presente nesta chamada, ***request***, pode ser usado posteriormente para verficiar o estado desta operação. \n",
    "\n",
    "O recebimento de mensagens também pode ser não bloqueante, usando-se ***MPI_Irecv***. \n",
    "\n",
    "* **MPI_Send** (buffer,count,type,dest,tag,comm): envio bloqueante\n",
    "* **MPI_Isend** (buffer,count,type,dest,tag,comm,request): envio não-bloqueante\n",
    "* **MPI_Recv** (buffer,count,type,source,tag,comm,status): recebimento bloqueante\n",
    "* **MPI_Irecv** (buffer,count,type,source,tag,comm,request): recebimento não bloqueante\n",
    "\n",
    "\n",
    "Parâmetros:\n",
    "\n",
    "* ***Buffer***: endereço de memória da localização dos dados; geralmente é o endereço de uma variável.\n",
    "* ***Data Count***: número de elementos de dados do tipo especificado a serem enviados.\n",
    "* ***Data Type***: tipos pré-definidos ou definidos pelo usuário.\n",
    "* ***Destination***: indica o rank do processo a quem se destina a msg.\n",
    "* ***Source***: especifica o rank do processo emissor. MPI_ANY_SOURCE permite receber de qualquer tarefa.\n",
    "* ***Tag***: identificador atribuído (0..32767) pelo programador para identificar uma mensagem. Permite especificar a mensagem a receber. MPI_ANY_TAG permite receber qualquer mensagem.\n",
    "* ***Communicator***: indica o conjunto de processos a quem se destina a mensagem. Normalmente usa-se MPI_COMM_WORLD.\n",
    "* ***Status***: em C, é um ponteiro para uma estrutura MPI_Status. Ex. stat.MPI_SOURCE, stat.MPI_TAG, MPI_Get_count routine (núm. Bytes recebidos)\n",
    "* ***Request***: usado em operações não-bloqueantes, retorna um \"request number\", que pode ser usado posteriormente (em operações do tipo WAIT) para determinar o estado da operação.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0BQ4IsZrHYm"
   },
   "source": [
    "O exemplo de código a seguir ilustra o uso de primitivas para envio e recebimento de mensagens. Neste exemplo, o processo de rank 0 envia mensagens individuais para cada um dos demais processos. Cada um deles recebe a mensagem do rank0 e envia uma resposta.\n",
    "\n",
    "Como há apenas 1 mensagem de cada origem para cada destino, o campo ***tag*** pode ser o mesmo em todas as trasmissões. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MP-6QUvfhbg",
    "outputId": "73e523d8-be2c-4a78-969c-c62d5d9161b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sr.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile sr.c\n",
    "\n",
    "#include <sys/types.h>\n",
    "#include <unistd.h>\n",
    "#include <stdio.h>\n",
    "#include <string.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "// #define LEN 256\n",
    "#define LEN (MPI_MAX_PROCESSOR_NAME + 128)\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint i, rank, result, numtasks, namelen, msgtag, pid, pid_0;\n",
    "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "\tchar tx_buf[LEN], rx_buf[LEN];\n",
    "\tMPI_Status status;\n",
    "\n",
    "\tresult = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (result != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
    "\t}\n",
    "\n",
    "\t// Determina número de processos em execução na aplicação\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    "\n",
    "\t// Determina ranking desse processo no grupo\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "\t// Determina nome do host local\n",
    "\tMPI_Get_processor_name(processor_name,&namelen);\n",
    "\n",
    "\tpid=getpid();\n",
    "\n",
    "\tmsgtag=1;\n",
    "\n",
    "\tif(rank==0) { // master node envia msg para todos os demais: 1..N-1\n",
    "\n",
    "\t\tfor(i=1; i < numtasks; i++) {\n",
    "\n",
    "\t\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm)\n",
    "\t\t\tMPI_Send (&pid, 1, MPI_INT,i,msgtag,MPI_COMM_WORLD);\n",
    "\t\t\t// printf(\"%s enviou: %d para processo %d\\n\",processor_name,pid, i);\n",
    "\t\t}\n",
    "\n",
    "\t\t// rank 0 aguarda resposta individual de cada um dos demais nós: 1..N-1\n",
    "\n",
    "\t\tfor(i=1; i < numtasks; i++) {\n",
    "\t\t\t// rank 0 recebe dos demais (MPI_Comm_size -1)\n",
    "   \n",
    "\t\t\t// int MPI_Recv(void *buf, int count, MPI_Datatype dtype, \n",
    "\t\t\t//              int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
    "\t\t\tMPI_Recv(rx_buf,LEN,MPI_CHAR, i, msgtag, MPI_COMM_WORLD,&status);\n",
    "\n",
    "\t\t\tprintf(\"%s: msg de resposta recebida do processo %d: %s\\n\", processor_name, i,rx_buf);\n",
    "\t\t}\n",
    "\n",
    "\t} else {   // worker nodes: todos recebem de rank 0 e retornam\n",
    "         \n",
    "\t\t// int MPI_Recv(void* buf,int count,MPI_Datatype datatype,\n",
    "\t\t//              int source, int tag,MPI_Comm comm,MPI_Status *status);\n",
    "\t\tMPI_Recv(&pid_0,1,MPI_INT,0,msgtag,MPI_COMM_WORLD,&status);\n",
    "\t\t// printf(\"%s recebeu: %d\\n\",processor_name,pid_0);\n",
    "\n",
    "\t\t// Ranks != 0 enviam msg para rank 0\n",
    "    // Monta mensagem de texto para resposta\n",
    "\t\tsprintf(tx_buf,\"%s: rank=%d, pid 0=%d\",processor_name,rank,pid_0);\n",
    "\n",
    "\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
    "\t\t//              int tag, MPI_Comm comm)\n",
    "\t\tMPI_Send(tx_buf,strlen(tx_buf)+1,MPI_CHAR,0,msgtag,MPI_COMM_WORLD);\n",
    "\t}\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vPK4zi4giB9",
    "outputId": "d1b1e046-f8e4-4fd4-8da8-8390b8d747c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3c1f42ea217e: msg de resposta recebida do processo 1: 3c1f42ea217e: rank=1, pid 0=33042\n",
      "3c1f42ea217e: msg de resposta recebida do processo 2: 3c1f42ea217e: rank=2, pid 0=33042\n",
      "3c1f42ea217e: msg de resposta recebida do processo 3: 3c1f42ea217e: rank=3, pid 0=33042\n"
     ]
    }
   ],
   "source": [
    "!mpicc sr.c -o sr && mpirun --allow-run-as-root -n 4 -host localhost:4 sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhDygKaQe2pH"
   },
   "source": [
    "O exemplo a seguir, baseado em https://mpitutorial.com/tutorials/mpi-send-and-receive/, ilustra um modelo de comunicação circular entre os processos da aplicação, também usando as primitivas MPI_Send e MPI_Recv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5oKx93wVzip",
    "outputId": "3bcd199f-d64b-4e5f-8aca-e6b533f797f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline.c\n",
    "\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <unistd.h>\n",
    "int length;\n",
    "char hostname[MPI_MAX_PROCESSOR_NAME];\n",
    "\n",
    "int \n",
    "main(int argc, char** argv) \n",
    "{\n",
    "  int world_rank;\n",
    "  int world_size;\n",
    "  int token, prox, ant;\n",
    "\n",
    "  MPI_Init(NULL, NULL);\n",
    "//obtem o nome do host em execucao\n",
    "  MPI_Get_processor_name(hostname, &length);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
    "     \n",
    "  // Recebe mensagem do processo de rank anterior e envia para o posterior\n",
    "  // Atenção com o primeiro e o último...\n",
    "  // Todas as mensagens são enviadas com Tag=0\n",
    "\n",
    "  prox = (world_rank +1) % world_size;\n",
    "  ant = (world_rank + world_size -1) % world_size;\n",
    " \n",
    "  // Quem começa? \n",
    "  if (world_rank == 0) {\n",
    "    token = 0;\n",
    "    // envia token: 1 valor do tipo MPI_INT\n",
    "    MPI_Send(&token, 1, MPI_INT, prox, 0, MPI_COMM_WORLD);\n",
    "  }\n",
    "  // todos agora, inclusive o 0...\n",
    "  do {\n",
    "    // espera token...\n",
    "    MPI_Recv(&token, 1, MPI_INT, ant, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    printf(\"Rank %d recebeu token %d do rank %d do node - %s\\n\", world_rank, token, ant, hostname);\n",
    "  \n",
    "    if(token < 3 * world_size -1) {\n",
    "      // Já que tem o token, poderia usar o recurso agora!\n",
    "      // Token, na verdade, poderia ser dados que serão manipulados localmente e\n",
    "      // encaminhados para mais processamentos no próximo nó doo pipeline...\n",
    "      sleep(rand()%3);\n",
    "      \n",
    "      token++;\n",
    "    }\n",
    "    // libera o token, passando-o para o próximo no anel\n",
    "    MPI_Send(&token, 1, MPI_INT, prox, 0, MPI_COMM_WORLD);\n",
    " \n",
    "  } while (token < 3 * world_size -1);\n",
    "  \n",
    "  printf(\"Rank %d terminando...\\n\",world_rank);\n",
    "  \n",
    "  MPI_Finalize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aI7gBVC2Cuxx",
    "outputId": "2e74973a-6fc0-4f6c-806e-7be32559fa45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job <pipeline> has been submitted to Intel(R) DevCloud with  nodes and will execute soon.\n",
      "\n",
      " If you do not see result in 60 seconds, please restart the Jupyter kernel:\n",
      " Kernel -> 'Restart Kernel and Clear All Outputs...' and then try again\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2237481.v-qsvr-1           ...ub-singleuser u187015         00:00:18 R jupyterhub     \n",
      "2237499.v-qsvr-1           compile.sh       u187015                0 Q batch          \n",
      "\n",
      "Waiting for Output ████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Tue 07 Mar 2023 02:31:39 PM PST\n",
      "#    Job ID:           2237499.v-qsvr-1.aidevcloud\n",
      "#      User:           u187015\n",
      "# Resources:           cput=75:00:00,neednodes=1:ppn=2,nodes=1:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "## u187015 is compiling ...\n",
      "\n",
      "Building script pipeline..\n",
      "Script pipeline built !\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2237499.v-qsvr-1.aidevcloud\n",
      "# Date: Tue 07 Mar 2023 02:31:47 PM PST\n",
      "########################################################################\n",
      "\n",
      "Job Completed in 20 seconds.\n",
      "Job <pipeline> has been submitted to Intel(R) DevCloud with  nodes and will execute soon.\n",
      "\n",
      " If you do not see result in 60 seconds, please restart the Jupyter kernel:\n",
      " Kernel -> 'Restart Kernel and Clear All Outputs...' and then try again\n",
      "\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "2237481.v-qsvr-1           ...ub-singleuser u187015         00:00:18 R jupyterhub     \n",
      "2237500.v-qsvr-1           launch.sh        u187015                0 Q batch          \n",
      "\n",
      "Waiting for Output ███████████████████████ Done⬇\n",
      "\n",
      "########################################################################\n",
      "#      Date:           Tue 07 Mar 2023 02:32:00 PM PST\n",
      "#    Job ID:           2237500.v-qsvr-1.aidevcloud\n",
      "#      User:           u187015\n",
      "# Resources:           cput=75:00:00,neednodes=2:ppn=2,nodes=2:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "## u187015 is compiling ...\n",
      "\n",
      "Building script pipeline..\n",
      "Rank 1 recebeu token 0 do rank 0 do host - s001-n009\n",
      "Rank 2 recebeu token 1 do rank 1 do host - s001-n012\n",
      "Rank 3 recebeu token 2 do rank 2 do host - s001-n012\n",
      "Rank 0 recebeu token 3 do rank 3 do host - s001-n009\n",
      "Rank 1 recebeu token 4 do rank 0 do host - s001-n009\n",
      "Rank 2 recebeu token 5 do rank 1 do host - s001-n012\n",
      "Rank 3 recebeu token 6 do rank 2 do host - s001-n012\n",
      "Rank 0 recebeu token 7 do rank 3 do host - s001-n009\n",
      "Rank 1 recebeu token 8 do rank 0 do host - s001-n009\n",
      "Rank 2 recebeu token 9 do rank 1 do host - s001-n012\n",
      "Rank 0 recebeu token 11 do rank 3 do host - s001-n009\n",
      "Rank 0 terminando...\n",
      "Rank 3 recebeu token 10 do rank 2 do host - s001-n012\n",
      "Rank 3 terminando...\n",
      "Rank 1 recebeu token 11 do rank 0 do host - s001-n009\n",
      "Rank 1 terminando...\n",
      "Rank 2 recebeu token 11 do rank 1 do host - s001-n012\n",
      "Rank 2 terminando...\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 2237500.v-qsvr-1.aidevcloud\n",
      "# Date: Tue 07 Mar 2023 02:32:18 PM PST\n",
      "########################################################################\n",
      "\n",
      "Job Completed in 23 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Aqui, testamos com 4 processos no mesmo nó. Se houver mais nós bastaria configurá-los no hostfile, ou especificar em linha de comando\n",
    "# ! mpicc -Wall pipeline.c -o pipeline && mpirun --allow-run-as-root -n 4 -host localhost:4 pipeline\n",
    "!./q compile.sh pipeline;\n",
    "!./q launch.sh pipeline nodes=2:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZGqIadihpNp"
   },
   "source": [
    "# MPI_ANY_SOURCE e MPI_ANY_TAG\n",
    "\n",
    "No exemplo a seguir, o processo de rank 0 envia e recebe mensagens para/de todos os demais processos da aplicação (MPI_COMM_WORLD).\n",
    "\n",
    "Neste caso, contudo, o recebimento pode ser fora de ordem; ou seja, o identificador do processo que enviou a próxima mensagem na fila não é conhecido previamente. Assim, uma opção é usar-se a constante **MPI_ANY_SOURCE** para identificar a origem da mensagem esperada. Como o **tag** pode variar também, usa-se a constante **MPI_ANY_TAG** para especificar a mensagem.\n",
    "\n",
    "Também pode ocorrer de o número de itens recebidos na mensagem não ser conhecido previamente. \n",
    "\n",
    "Vale observar que na operação de recebimento há um parâmetro a mais que no envio, que é um ponteiro para uma estrutura **MPI_Status**. No retorno da chamada, esssa variável vai estar preenchida com informações sobre a mensagem recebida. \n",
    "\n",
    "```\n",
    "MPI_Status {\n",
    "\tint MPI_SOURCE;\n",
    "\tint MPI_TAG;\n",
    "\tint MPI_ERROR;\n",
    "\tint st_length;  // message length // tamanho da mensagem recebida\n",
    "};\n",
    "```\n",
    "Os campos MPI_SOURCE, MPI_TAG e MPI_ERROR podem ser usados diretamente. Entretanto, para saber o número de itens recebidos na mensagem é preciso usar a função [MPI_Get_count](https://www.open-mpi.org/doc/v3.1/man3/MPI_Get_count.3.php), indicando o tipo dos itens contidos na mensagem:\n",
    "\n",
    "```\n",
    "int MPI_Get_count(const MPI_Status *status, MPI_Datatype datatype, int *count);\n",
    "```\n",
    "O exemplo a seguir ilustra essa forma de recebimento, com identificação do emissor poteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBoMQ9wRCOxf",
    "outputId": "a6b5b8c2-1ebe-471f-b08a-274f37768042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sr-any.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile sr-any.c\n",
    "\n",
    "#include <sys/types.h>\n",
    "#include <unistd.h>\n",
    "#include <stdio.h>\n",
    "#include <string.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "#define LEN 256\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint i, rank, count, result, numtasks, namelen, msgtag, pid, pid_0;\n",
    "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "\tchar tx_buf[LEN], rx_buf[LEN];\n",
    "\tMPI_Status status;\n",
    "\n",
    "\tresult = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (result != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
    "\t}\n",
    "\n",
    "\t// Determina número de processos em execução na aplicação\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    "\n",
    "\t// Determina ranking desse processo no grupo\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "\t// Determina nome do host local\n",
    "\tMPI_Get_processor_name(processor_name,&namelen);\n",
    "\n",
    "\tpid=getpid();\n",
    "\n",
    "\tmsgtag=1;\n",
    "\n",
    "\t// todos os processos executaram o mesmo código até aqui!\n",
    "\n",
    "\tif(rank==0) { // master node\n",
    "\n",
    "\t\t// rank 0 envia valor de seu PID para todos os demais: 1..N-1\n",
    "\t\tfor(i=1; i < numtasks; i++) {\n",
    "\t\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm)\n",
    "\t\t\tMPI_Send(&pid,1,MPI_INT,i,msgtag,MPI_COMM_WORLD);\n",
    "\t\t\t// printf(\"%s enviou: %d\\n\",processor_name,pid);\n",
    "\t\t}\n",
    "\n",
    "\t\t// rank 0 aguarda resposta individual de cada um dos demais nós: 1..N-1\n",
    "\n",
    "\t\tfor(i=1;i<numtasks;i++) {\n",
    "\t\t\t// rank 0 recebe dos demais (MPI_Comm_size -1)\n",
    "   \n",
    "\t\t\t// Uso de MPI_ANY_SOURCE e MPI_ANY_TAG: não se sabe a ordem de envio\n",
    "\t\t\t// int MPI_Recv(void *buf, int count, MPI_Datatype dtype, \n",
    "\t\t\t//              int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
    "\t\t\tMPI_Recv(rx_buf,LEN,MPI_CHAR,MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,&status);\n",
    "\n",
    "      // Neste caso, como os valores MPI_ANY_SOURCE e MPI_ANY_TAG foram usados,\n",
    "      // se for preciso identificar o emissor, é preciso verificar as informações\n",
    "      // retornadas na estrutura status. \n",
    "\n",
    "\t\t\t// Campos de MPI_Status. Tamanho da mensagem recebida pode ser consultado via MPI_Get_count\n",
    "\t\t\t// MPI_Status {\n",
    "\t\t\t//    int MPI_SOURCE;\n",
    "\t\t\t//    int MPI_TAG;\n",
    "\t\t\t//    int MPI_ERROR;\n",
    "\t\t\t//    int st_length;  // message length \n",
    "\t\t\t// };\n",
    "\n",
    "\t\t\t// int MPI_Get_count(const MPI_Status *status, MPI_Datatype datatype, int *count);\n",
    "\t\t\t// Retorna o número de itens recebidos\n",
    "\t\t\tresult = MPI_Get_count(&status, MPI_CHAR, &count);\n",
    "\n",
    "\t\t\tprintf(\"%d @ %s recebeu %d itens do processo %d (%d): %s\\n\",\n",
    "          rank, processor_name, count, status.MPI_SOURCE, status.MPI_TAG, rx_buf);\n",
    "\t\t}\n",
    "\n",
    "\t} else {   // worker nodes\n",
    "\n",
    "\t\t// todos recebem de rank 0\n",
    "\t\t// int MPI_Recv(void* buf,int count,MPI_Datatype datatype,\n",
    "\t\t//              int source, int tag,MPI_Comm comm,MPI_Status *status);\n",
    "\t\tMPI_Recv(&pid_0,1,MPI_INT,0,msgtag,MPI_COMM_WORLD,&status);\n",
    "\t\t// printf(\"%s recebeu: %d\\n\",processor_name,pid_0);\n",
    "\n",
    "\t\t// Ranks != 0 enviam msg para rank 0\n",
    "\t\tsprintf(tx_buf,\"%s: rank=%d, pid 0=%d\",processor_name,rank,pid_0);\n",
    "\n",
    "\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
    "\t\t//              int tag, MPI_Comm comm)\n",
    "\t\tmsgtag=pid;\n",
    "\t\tMPI_Send(tx_buf,strlen(tx_buf)+1,MPI_CHAR,0,msgtag,MPI_COMM_WORLD);\n",
    "\t\t// MPI_Send(&pid,1,MPI_INT,0,msgtag,MPI_COMM_WORLD);\n",
    "\n",
    "\t}\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KjHhDG9Tj-8",
    "outputId": "a809d8bb-0828-4015-db9d-7fafb4e79783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01m\u001b[Ksr-any.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksr-any.c:88:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K: rank=\u001b[m\u001b[K’ directive writing 7 bytes into a region of size between 1 and 256 [\u001b[01;35m\u001b[K-Wformat-overflow=\u001b[m\u001b[K]\n",
      "   88 |   sprintf(tx_buf,\"%s\u001b[01;35m\u001b[K: rank=\u001b[m\u001b[K%d, pid 0=%d\",processor_name,rank,pid_0);\n",
      "      |                     \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksr-any.c:88:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ksprintf\u001b[m\u001b[K’ output between 18 and 293 bytes into a destination of size 256\n",
      "   88 |   \u001b[01;36m\u001b[Ksprintf(tx_buf,\"%s: rank=%d, pid 0=%d\",processor_name,rank,pid_0)\u001b[m\u001b[K;\n",
      "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "0 @ 3c1f42ea217e recebeu 34 itens do processo 3 (12305): 3c1f42ea217e: rank=3, pid 0=12300\n",
      "0 @ 3c1f42ea217e recebeu 34 itens do processo 1 (12301): 3c1f42ea217e: rank=1, pid 0=12300\n",
      "0 @ 3c1f42ea217e recebeu 34 itens do processo 2 (12302): 3c1f42ea217e: rank=2, pid 0=12300\n"
     ]
    }
   ],
   "source": [
    "!mpicc -Wall sr-any.c -o sr-any && mpirun --allow-run-as-root -n 4 -host localhost:4 sr-any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ_6GIUSm0aT"
   },
   "source": [
    "# MPI_Probe\n",
    "\n",
    "(baseado em https://mpitutorial.com/tutorials/dynamic-receiving-with-mpi-probe-and-mpi-status/)\n",
    "\n",
    "\n",
    "Em muitas aplicações distribuídas, além de o nó receptor não saber previamente o emissor das mensagens, o que o obriga a usar a origem MPI_ANY_SOURCE, é comum que mesmo o tamanho das mensagens não seja conhecido.\n",
    "\n",
    "Neste caso, é possível alocar um *buffer* com o tamanho da maior mensagem prevista. \n",
    "\n",
    "Entretanto, como as mensagens transmitidas via *socket* pela implementação MPI são armazenadas previamente em *buffers* locais, é possível à aplicação saber previamente o tamanho de uma messagem já recebida pela API, antes chamar a operação MPI_Recv.\n",
    "\n",
    "Essa consulta pode ser feita com a chamada MPI_Probe:\n",
    "```\n",
    "MPI_Probe(int source, int tag, MPI_Comm comm, MPI_Status* status)\n",
    "```\n",
    "De maneira equivalente a MPI_Recv, MPI_Probe bloqueia à espera do recebimento de uma mensagem específica pela implementação da API, sem que o conteúdo seja copiado para algum endereço especificado pela aplicação, contudo. \n",
    "\n",
    "Quando a chamada retorna, a estrutura *statud* pode ser usada com a função MPI_Get_count para saber o número de itens recebidos. É possível então alocar um buffer de tamanho apropriado para o recebimento.\n",
    "```\n",
    "int number_amount;\n",
    "\n",
    "if (world_rank == 0) {\n",
    "    const int MAX_NUMBERS = 100;\n",
    "    int numbers[MAX_NUMBERS];\n",
    "\n",
    "    // Pick a random amount of integers to send to process one\n",
    "    srand(time(NULL));\n",
    "    number_amount = (rand() / (float)RAND_MAX) * MAX_NUMBERS;\n",
    "\n",
    "    // Send the random amount of integers to process one\n",
    "    MPI_Send(numbers, number_amount, MPI_INT, 1, 0, MPI_COMM_WORLD);\n",
    "    printf(\"0 sent %d numbers to 1\\n\", number_amount);\n",
    "\n",
    "} else if (world_rank == 1) {\n",
    "    MPI_Status status;\n",
    "    // Probe for an incoming message from process zero\n",
    "    MPI_Probe(0, 0, MPI_COMM_WORLD, &status);\n",
    "\n",
    "    // When probe returns, the status object has the size and other\n",
    "    // attributes of the incoming message. Get the message size\n",
    "    MPI_Get_count(&status, MPI_INT, &number_amount);\n",
    "\n",
    "    // Allocate a buffer to hold the incoming numbers\n",
    "    int* number_buf = (int*)malloc(sizeof(int) * number_amount);\n",
    "\n",
    "    // Now receive the message with the allocated buffer\n",
    "    MPI_Recv(number_buf,number_amount,MPI_INT,0,0,MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    printf(\"1 dynamically received %d numbers from 0.\\n\", number_amount);\n",
    "    free(number_buf);\n",
    "}\n",
    "```\n",
    "\n",
    "Se a locação e a liberação de memória forem constantes no código, contudo, talvez valha a pena usar um buffer de tamanho máximo :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJCyZ2twdp-G",
    "outputId": "71431980-9564-4f13-ca23-38587ae180f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sr-any.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile sr-any.c\n",
    "\n",
    "#include <sys/types.h>\n",
    "#include <unistd.h>\n",
    "#include <stdio.h>\n",
    "#include <string.h>\n",
    "#include <unistd.h>\n",
    "#include <stdlib.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "#define LEN 256\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint i, rank, result, numtasks, namelen, msgtag;\n",
    "  double local_time, remote_time, mean_time, time_diff;\n",
    "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    " \n",
    "\tMPI_Status status;\n",
    "\n",
    "\tresult = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (result != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro iniciando programa MPI.\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, result);\n",
    "\t}\n",
    "\n",
    "\t// Determina número de processos em execução na aplicação\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    "\n",
    "\t// Determina ranking desse processo no grupo\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "\t// Determina nome do host local\n",
    "\tMPI_Get_processor_name(processor_name,&namelen);\n",
    "\n",
    "\tmsgtag=1;\n",
    "\n",
    "\tif(rank==0) { // master node\n",
    "\n",
    "\t\t// rank 0 envia o instante local para todos os demais: 1..N-1\n",
    "\t\tfor(i=1; i < numtasks; i++) {\n",
    "\n",
    "      // determina instante atual\n",
    "      local_time = MPI_Wtime();\n",
    "\n",
    "\t\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest, int tag, MPI_Comm comm)\n",
    "\t\t\tMPI_Send(&local_time, 1, MPI_DOUBLE, i, msgtag, MPI_COMM_WORLD);\n",
    "\n",
    "\t\t\t// printf(\"%s enviou: %f\\n\",processor_name, local_time);\n",
    "\t\t}\n",
    "\n",
    "\t\t// rank 0 aguarda resposta individual de cada um dos demais nós: 1..N-1\n",
    "\n",
    "    mean_time = 0.0;\n",
    "\n",
    "\t\tfor(i=1; i < numtasks;i++) {\n",
    "\n",
    "\t\t\t// rank 0 recebe dos demais (MPI_Comm_size -1)\n",
    "   \n",
    "\t\t\t// Uso de MPI_ANY_SOURCE e MPI_ANY_TAG: não se sabe a ordem de envio\n",
    "\t\t\t// int MPI_Recv(void *buf, int count, MPI_Datatype dtype, \n",
    "\t\t\t//              int src, int tag, MPI_Comm comm, MPI_Status *stat)\n",
    "\t\t\tMPI_Recv(&remote_time,1,MPI_DOUBLE, MPI_ANY_SOURCE,MPI_ANY_TAG,MPI_COMM_WORLD,&status);\n",
    "\n",
    "      mean_time += remote_time;\n",
    "\n",
    "\t\t\tprintf(\"%d @ %s recebeu time_diff do processo %d: %f\\n\",\n",
    "          rank, processor_name, status.MPI_SOURCE, remote_time);\n",
    "\t\t}\n",
    "    mean_time /= numtasks;\n",
    "\n",
    "    printf(\"Atraso médio de propagaçao: %f\\n\", mean_time);\n",
    "\n",
    "\n",
    "\t} else {   // worker nodes\n",
    "\n",
    "\t\t// todos recebem de rank 0\n",
    "\t\t// int MPI_Recv(void* buf,int count,MPI_Datatype datatype,\n",
    "\t\t//              int source, int tag,MPI_Comm comm,MPI_Status *status);\n",
    "\t\tMPI_Recv(&remote_time,1,MPI_DOUBLE,0,msgtag,MPI_COMM_WORLD,&status);\n",
    "\n",
    "\t\t// printf(\"%s recebeu: %f\\n\",processor_name,remote_time);\n",
    "\n",
    "    local_time = MPI_Wtime();    \n",
    "    time_diff = local_time - remote_time;\n",
    "\n",
    "    // dorme um pouquinho, para gerar envio em ordem aleatória...\n",
    "    usleep(rand()%100);\n",
    "\n",
    "\t\t// int MPI_Send(void *buf, int count, MPI_Datatype dtype, int dest,\n",
    "\t\t//              int tag, MPI_Comm comm)\n",
    "\t\tMPI_Send(&time_diff, 1, MPI_DOUBLE,0,msgtag,MPI_COMM_WORLD);\n",
    "\t}\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEQcY60C5WRE"
   },
   "source": [
    "O exemplo a seguir ilustra a comunicação não bloqueante com as primitivas MPI_Isnd e MPI_Irecv. Essas chamadas liberam a biblioteca MPI para escrever nos *buffers* internos associados às transmissões. Posteriormente, é possível   bloquear o prosseguimento do programa até que as operações tenham sido concluídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSRD029k5W4U",
    "outputId": "9bc6531d-5390-4f3f-9cfc-edcc17dad528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing isnd.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile isnd.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "#include \"mpi.h\"\n",
    "\n",
    "int \n",
    "main(int argc,char *argv[])\n",
    "{ \n",
    "  int numtasks, rank, next, prev, buf[2], tag1=1, tag2=2;\n",
    "  MPI_Request reqs[4];   // required variable for non-blocking calls\n",
    "  MPI_Status stats[4];   // required variable for Waitall routine\n",
    "\n",
    "  char hostname[MPI_MAX_PROCESSOR_NAME];\n",
    "  int namelen;\n",
    "\n",
    "\n",
    "  MPI_Init(&argc,&argv);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "  // Determina nome do host local\n",
    "  MPI_Get_processor_name(hostname,&namelen);\n",
    "\n",
    "  // determina nós vizinhos à esquerda e à direita\n",
    "  prev = rank-1;\n",
    "  if (rank == 0)\n",
    "    prev = numtasks - 1;\n",
    " \n",
    "  next = rank+1;\n",
    "  if (rank == (numtasks - 1))\n",
    "    next = 0;\n",
    "\n",
    "  // int MPI_Irecv(void *buf, int count, MPI_Datatype datatype,\n",
    "  //               int source, int tag, MPI_Comm comm, MPI_Request *request);\n",
    "  // \n",
    "  // Nonblocking calls allocate a communication request object and associate it \n",
    "  // with the request handle (the argument request). The request can be used \n",
    "  // later to query the status of the communication or wait for its completion.\n",
    "  // \n",
    "  // A nonblocking receive call indicates that the system may start writing data \n",
    "  // into the receive buffer. The receiver should not access any part of the \n",
    "  // receive buffer after a nonblocking receive operation is called, until the \n",
    "  // receive completes.\n",
    "  // \n",
    "  // A receive request can be determined being completed by calling the MPI_Wait, \n",
    "  // MPI_Waitany, MPI_Test, or MPI_Testany with request returned by this function. \n",
    "\n",
    "  // post non-blocking receives and sends for neighbors\n",
    "  MPI_Irecv(&buf[0], 1, MPI_INT, prev, tag1, MPI_COMM_WORLD, &reqs[0]);\n",
    "  MPI_Irecv(&buf[1], 1, MPI_INT, next, tag2, MPI_COMM_WORLD, &reqs[1]);\n",
    "\n",
    "  // int MPI_Isend(const void* buf, int count, MPI_Datatype datatype, int dest,\n",
    "  //               int tag, MPI_Comm comm, MPI_Request *request);\n",
    "  // \n",
    "  // MPI_Isend starts a standard-mode, nonblocking send. Nonblocking calls \n",
    "  // allocate a communication request object and associate it with the request \n",
    "  // handle (the argument request). The request can be used later to query the \n",
    "  // status of the communication or wait for its completion.\n",
    "  // \n",
    "  // A nonblocking send call indicates that the system may start copying data \n",
    "  // out of the send buffer. The sender should not modify any part of the send \n",
    "  // buffer after a nonblocking send operation is called, until the send completes.\n",
    "  // \n",
    "  // A send request can be determined being completed by calling the MPI_Wait, \n",
    "  // MPI_Waitany, MPI_Test, or MPI_Testany with request returned by this function. \n",
    "\n",
    "  MPI_Isend(&rank, 1, MPI_INT, prev, tag2, MPI_COMM_WORLD, &reqs[2]);\n",
    "  MPI_Isend(&rank, 1, MPI_INT, next, tag1, MPI_COMM_WORLD, &reqs[3]);\n",
    "\n",
    "\n",
    "  // aqui, devereia haver algo últil a fazer, ao invés de parar à espera das mensagens...\n",
    "/ message length // tamanho da mensagem recebida\n",
    "};\n",
    "\n",
    "  // int MPI_Waitall(int count, MPI_Request array_of_requests[],\n",
    "  //                 MPI_Status *array_of_statuses)\n",
    "  // \n",
    "  // Blocks until all communication operations associated with active handles in \n",
    "  // the list complete, and returns the status of all these operations. Both \n",
    "  // arrays have the same number of valid entries. The ith entry in array_of_statuses \n",
    "  // is set to the return status of the ith operation. Requests that were created by \n",
    "  // nonblocking communication operations are deallocated, and the corresponding\n",
    "  // handles in the array are set to MPI_REQUEST_NULL. \n",
    "  // \n",
    "  // When one or more of the communications completed by a call to MPI_Waitall \n",
    "  // fail, it is desirable to return specific information on each communication. \n",
    "\n",
    "  MPI_Waitall(4, reqs, stats);\n",
    "\n",
    "  printf(\"%s (%d): buf[0]: %d, buf[1]: %d\\n\",hostname,rank,buf[0],buf[1]);\n",
    "\n",
    "  MPI_Finalize();\n",
    "\n",
    "  return(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNJ14ALn5YKl",
    "outputId": "0ce4ba44-cc41-446c-f3e0-28c87fcc0861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745d0edadf06 (0): buf[0]: 3, buf[1]: 1\n",
      "745d0edadf06 (1): buf[0]: 0, buf[1]: 2\n",
      "745d0edadf06 (3): buf[0]: 2, buf[1]: 0\n",
      "745d0edadf06 (2): buf[0]: 1, buf[1]: 3\n"
     ]
    }
   ],
   "source": [
    "!mpicc -Wall isnd.c -o isnd && mpirun --allow-run-as-root -n 4 -host localhost:4 isnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4Otl3Lk-3cE"
   },
   "source": [
    "Ainda sobre o exemplo anterior, já que cada processo realiza operações e transmissão e recebimento, seria possível ainda substituir essas 2 operações por uma uma chamada a MPI_Sendrecv:\n",
    "\n",
    "```\n",
    "int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype,\n",
    "    int dest, int sendtag, void *recvbuf, int recvcount,\n",
    "    MPI_Datatype recvtype, int source, int recvtag,\n",
    "    MPI_Comm comm, MPI_Status *status)\n",
    "```\n",
    "Esta chamada permite enviar e receber mensagens de nós distintos, com tags, tipos e tamanhos distintos também.\n",
    "\n",
    "Bem, há várias primitivas de comunicação com MPI. Espero que os exemplos tratados sirvam para entender os mecanismos utilizados e para explorar outras funcionalidades!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oqLeez1BbscH",
    "t4kAk0w6SW1P"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
